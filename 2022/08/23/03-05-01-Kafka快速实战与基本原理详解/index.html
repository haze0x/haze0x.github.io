<!DOCTYPE html>


<html lang="en">
  

    <head>
      <meta charset="utf-8" />
        
      <meta
        name="viewport"
        content="width=device-width, initial-scale=1, maximum-scale=1"
      />
      <title>Kafka快速实战与基本原理详解 |  Sword Heart</title>
  <meta name="generator" content="hexo-theme-ayer">
      
      <link rel="shortcut icon" href="/favicon.ico" />
       
<link rel="stylesheet" href="/dist/main.css">

      <link
        rel="stylesheet"
        href="https://cdn.jsdelivr.net/gh/Shen-Yu/cdn/css/remixicon.min.css"
      />
      
<link rel="stylesheet" href="/css/custom.css">
 
      <script src="https://cdn.jsdelivr.net/npm/pace-js@1.0.2/pace.min.js"></script>
       
 

      <link
        rel="stylesheet"
        href="https://cdn.jsdelivr.net/npm/@sweetalert2/theme-bulma@5.0.1/bulma.min.css"
      />
      <script src="https://cdn.jsdelivr.net/npm/sweetalert2@11.0.19/dist/sweetalert2.min.js"></script>

      <!-- mermaid -->
      
      <style>
        .swal2-styled.swal2-confirm {
          font-size: 1.6rem;
        }
      </style>
    <link rel="alternate" href="/atom.xml" title="Sword Heart" type="application/atom+xml">
</head>
  </html>
</html>


<body>
  <div id="app">
    
      
      <canvas width="1777" height="841"
        style="position: fixed; left: 0px; top: 0px; z-index: 99999; pointer-events: none;"></canvas>
      
    <main class="content on">
      <section class="outer">
  <article
  id="post-03-05-01-Kafka快速实战与基本原理详解"
  class="article article-type-post"
  itemscope
  itemprop="blogPost"
  data-scroll-reveal
>
  <div class="article-inner">
    
    <header class="article-header">
       
<h1 class="article-title sea-center" style="border-left:0" itemprop="name">
  Kafka快速实战与基本原理详解
</h1>
 

      
    </header>
     
    <div class="article-meta">
      <a href="/2022/08/23/03-05-01-Kafka%E5%BF%AB%E9%80%9F%E5%AE%9E%E6%88%98%E4%B8%8E%E5%9F%BA%E6%9C%AC%E5%8E%9F%E7%90%86%E8%AF%A6%E8%A7%A3/" class="article-date">
  <time datetime="2022-08-23T15:13:59.000Z" itemprop="datePublished">2022-08-23</time>
</a> 
  <div class="article-category">
    <a class="article-category-link" href="/categories/%E4%B8%AD%E9%97%B4%E4%BB%B6/">中间件</a> / <a class="article-category-link" href="/categories/%E4%B8%AD%E9%97%B4%E4%BB%B6/%E5%88%86%E5%B8%83%E5%BC%8F/">分布式</a> / <a class="article-category-link" href="/categories/%E4%B8%AD%E9%97%B4%E4%BB%B6/%E5%88%86%E5%B8%83%E5%BC%8F/kafka/">kafka</a>
  </div>
  
<div class="word_count">
    <span class="post-time">
        <span class="post-meta-item-icon">
            <i class="ri-quill-pen-line"></i>
            <span class="post-meta-item-text"> Word count:</span>
            <span class="post-count">7.7k</span>
        </span>
    </span>

    <span class="post-time">
        &nbsp; | &nbsp;
        <span class="post-meta-item-icon">
            <i class="ri-book-open-line"></i>
            <span class="post-meta-item-text"> Reading time≈</span>
            <span class="post-count">32 min</span>
        </span>
    </span>
</div>
 
    </div>
      
    <div class="tocbot"></div>




  
    <div class="article-entry" itemprop="articleBody">
       
  <p id="hitokoto">正在加载一言...</p>

<h2 id="Kafka快速实战与基本原理详解"><a href="#Kafka快速实战与基本原理详解" class="headerlink" title="Kafka快速实战与基本原理详解"></a>Kafka快速实战与基本原理详解</h2><h3 id="1、Kafka的使用场景"><a href="#1、Kafka的使用场景" class="headerlink" title="1、Kafka的使用场景"></a>1、<strong>Kafka的使用场景</strong></h3><ul>
<li>日志收集：一个公司可以用Kafka收集各种服务的log，通过kafka以统一接口服务的方式开放给各种consumer，例如hadoop、Hbase、Solr等。</li>
<li>消息系统：解耦和生产者和消费者、缓存消息等。</li>
<li>用户活动跟踪：Kafka经常被用来记录web用户或者app用户的各种活动，如浏览网页、搜索、点击等活动，这些活动信息被各个服务器发布到kafka的topic中，然后订阅者通过订阅这些topic来做实时的监控分析，或者装载到hadoop、数据仓库中做离线分析和挖掘。</li>
<li>运营指标：Kafka也经常用来记录运营监控数据。包括收集各种分布式应用的数据，生产各种操作的集中反馈，比如报警和报告。</li>
</ul>
<h3 id="2、Kafka基本概念"><a href="#2、Kafka基本概念" class="headerlink" title="2、Kafka基本概念"></a>2、<strong>Kafka基本概念</strong></h3><p>kafka是一个分布式的，分区的消息(官方称之为commit log)服务。它提供一个消息系统应该具备的功能，但是确有着独特的设计。可以这样来说，Kafka借鉴了JMS规范的思想，但是确并<strong>没有完全遵循JMS规范。</strong></p>
<p>首先，让我们来看一下基础的消息(Message)相关术语：</p>
<table>
<thead>
<tr>
<th><strong>名称</strong></th>
<th><strong>解释</strong></th>
</tr>
</thead>
<tbody><tr>
<td>Broker</td>
<td>消息中间件处理节点，一个Kafka节点就是一个broker，一个或者多个Broker可以组成一个Kafka集群</td>
</tr>
<tr>
<td>Topic</td>
<td>Kafka根据topic对消息进行归类，发布到Kafka集群的每条消息都需要指定一个topic逻辑概念，真正消息发送在partition</td>
</tr>
<tr>
<td>Producer</td>
<td>消息生产者，向Broker发送消息的客户端</td>
</tr>
<tr>
<td>Consumer</td>
<td>消息消费者，从Broker读取消息的客户端</td>
</tr>
<tr>
<td>ConsumerGroup</td>
<td>每个Consumer属于一个特定的Consumer Group，一条消息可以被多个不同的Consumer Group消费，但是一个Consumer Group中只能有一个Consumer能够消费该消息</td>
</tr>
<tr>
<td>Partition</td>
<td>物理上的概念，一个topic可以分为多个partition，每个partition内部消息是有序的</td>
</tr>
</tbody></table>
<p>因此，从一个较高的层面上来看，producer通过网络发送消息到Kafka集群，然后consumer来进行消费，如下图：</p>
<p><img src="/2022/08/23/03-05-01-Kafka%E5%BF%AB%E9%80%9F%E5%AE%9E%E6%88%98%E4%B8%8E%E5%9F%BA%E6%9C%AC%E5%8E%9F%E7%90%86%E8%AF%A6%E8%A7%A3/image-20220728013835791.png" alt="image-20220728013835791"></p>
<blockquote>
<p>服务端(brokers)和客户端(producer、consumer)之间通信通过<strong>TCP协议</strong>来完成。</p>
</blockquote>
<h3 id="3、kafka环境搭建"><a href="#3、kafka环境搭建" class="headerlink" title="3、kafka环境搭建"></a>3、kafka环境搭建</h3><ul>
<li><p>安装jdk</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">yum install java-<span class="number">1.8</span><span class="number">.0</span>-openjdk* -y</span><br></pre></td></tr></table></figure></li>
<li><p>kafka依赖zookeeper，所以需要先安装zookeeper</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">wget https:<span class="comment">//archive.apache.org/dist/zookeeper/zookeeper-3.5.8/apache-zookeeper-3.5.8-bin.tar.gz</span></span><br><span class="line">tar -zxvf apache-zookeeper-<span class="number">3.5</span><span class="number">.8</span>-bin.tar.gz</span><br><span class="line">cd  apache-zookeeper-<span class="number">3.5</span><span class="number">.8</span>-bin</span><br><span class="line">cp conf/zoo_sample.cfg conf/zoo.cfg</span><br><span class="line"></span><br><span class="line"># 启动zookeeper</span><br><span class="line">bin/zkServer.sh start</span><br><span class="line">bin/zkCli.sh </span><br><span class="line">ls /			#查看zk的根目录相关节点</span><br></pre></td></tr></table></figure></li>
<li><p><strong>下载安装包</strong></p>
<p>下载2.4.1 release版本，并解压：</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">wget https:<span class="comment">//archive.apache.org/dist/kafka/2.4.1/kafka_2.11-2.4.1.tgz  # 2.11是scala的版本，2.4.1是kafka的版本</span></span><br><span class="line">tar -xzf kafka_2<span class="number">.11</span>-<span class="number">2.4</span><span class="number">.1</span>.tgz</span><br><span class="line">cd kafka_2<span class="number">.11</span>-<span class="number">2.4</span><span class="number">.1</span></span><br></pre></td></tr></table></figure></li>
<li><p><strong>修改配置</strong></p>
<p>修改配置文件config/server.properties:</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">#broker.id属性在kafka集群中必须要是唯一</span><br><span class="line">broker.id=<span class="number">0</span></span><br><span class="line">#kafka部署的机器ip和提供服务的端口号</span><br><span class="line">listeners=PLAINTEXT:<span class="comment">//192.168.65.60:9092   </span></span><br><span class="line">#kafka的消息存储文件</span><br><span class="line">log.dir=/usr/local/data/kafka-logs</span><br><span class="line">#kafka连接zookeeper的地址</span><br><span class="line">zookeeper.connect=<span class="number">192.168</span><span class="number">.65</span><span class="number">.60</span>:<span class="number">2181</span></span><br></pre></td></tr></table></figure></li>
<li><p><strong>启动服务</strong></p>
<p>现在来启动kafka服务：</p>
<p>启动脚本语法：<code>kafka-server-start.sh [-daemon] server.properties</code>&gt;</p>
<p>可以看到，<code>server.properties</code>&gt;的配置路径是一个强制的参数，-daemon表示以后台进程运行，否则ssh客户端退出后，就会停止服务。(注意，在启动kafka时会使用linux主机名关联的ip地址，所以需要把主机名和linux的ip映射配置到本地host里，用vim /etc/hosts)</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">启动kafka，运行日志在logs目录的server.log文件里</span><br><span class="line">bin/kafka-server-start.sh -daemon config/server.properties   #后台启动，不会打印日志到控制台 </span><br><span class="line">或者用 </span><br><span class="line">bin/kafka-server-start.sh config/server.properties &amp; </span><br><span class="line">#我们进入zookeeper目录通过zookeeper客户端查看下zookeeper的目录树   </span><br><span class="line">bin/zkCli.sh  </span><br><span class="line">ls / #查看zk的根目录kafka相关节点 </span><br><span class="line">ls /brokers/ids	#查看kafka节点 </span><br><span class="line"># 停止kafka </span><br><span class="line">bin/kafka-server-stop.sh</span><br></pre></td></tr></table></figure>

<p>启动报错日志可以查看server.log</p>
<p><strong>server.properties核心配置详解：</strong></p>
<table>
<thead>
<tr>
<th><strong>Property</strong></th>
<th><strong>Default</strong></th>
<th><strong>Description</strong></th>
</tr>
</thead>
<tbody><tr>
<td>broker.id</td>
<td>0</td>
<td>每个broker都可以用一个唯一的非负整数id进行标识；这个id可以作为broker的“名字”，你可以选择任意你喜欢的数字作为id，只要id是唯一的即可。</td>
</tr>
<tr>
<td>log.dirs</td>
<td>/tmp/kafka-logs</td>
<td>kafka存放数据的路径。这个路径并不是唯一的，可以是多个，路径之间只需要使用逗号分隔即可；每当创建新partition时，都会选择在包含最少partitions的路径下进行。</td>
</tr>
<tr>
<td>listeners</td>
<td>PLAINTEXT://192.168.65.60:9092</td>
<td>server接受客户端连接的端口，ip配置kafka本机ip即可</td>
</tr>
<tr>
<td>zookeeper.connect</td>
<td>localhost:2181</td>
<td>zooKeeper连接字符串的格式为：hostname:port，此处hostname和port分别是ZooKeeper集群中某个节点的host和port；zookeeper如果是集群，连接方式为 hostname1:port1, hostname2:port2, hostname3:port3</td>
</tr>
<tr>
<td>log.retention.hours</td>
<td>168</td>
<td>每个日志文件删除之前保存的时间。默认数据保存时间对所有topic都一样。</td>
</tr>
<tr>
<td>num.partitions</td>
<td>1</td>
<td>创建topic的默认分区数</td>
</tr>
<tr>
<td>default.replication.factor</td>
<td>1</td>
<td>自动创建topic的默认副本数量，建议设置为大于等于2</td>
</tr>
<tr>
<td>min.insync.replicas</td>
<td>1</td>
<td>当producer设置acks为-1时，min.insync.replicas指定replicas的最小数目（必须确认每一个repica的写数据都是成功的），如果这个数目没有达到，producer发送消息会产生异常</td>
</tr>
<tr>
<td>delete.topic.enable</td>
<td>false</td>
<td>是否允许删除主题</td>
</tr>
</tbody></table>
</li>
<li><p><strong>创建主题</strong> </p>
<p>现在我们来创建一个名字为“test”的Topic，这个topic只有一个partition，并且备份因子也设置为1：</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">bin/kafka-topics.sh --create --zookeeper <span class="number">192.168</span><span class="number">.31</span><span class="number">.139</span>:<span class="number">2181</span> --replication-factor <span class="number">1</span> --partitions <span class="number">1</span> --topic test</span><br></pre></td></tr></table></figure>

<p>查看kafka中目前存在的topic</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">bin/kafka-topics.sh --list --zookeeper <span class="number">192.168</span><span class="number">.65</span><span class="number">.60</span>:<span class="number">2181</span></span><br></pre></td></tr></table></figure>

<p>除了我们通过手工的方式创建Topic，当producer发布一个消息到某个指定的Topic，这个Topic如果不存在，就自动创建。</p>
</li>
<li><p><strong>删除主题</strong></p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">bin/kafka-topics.sh --delete --topic test --zookeeper <span class="number">192.168</span><span class="number">.65</span><span class="number">.60</span>:<span class="number">2181</span></span><br></pre></td></tr></table></figure></li>
<li><p><strong>发送消息</strong></p>
<p>kafka自带了一个producer命令客户端，可以从本地文件中读取内容，或者我们也可以以命令行中直接输入内容，并将这些内容以消息的形式发送到kafka集群中。在默认情况下，每一个行会被当做成一个独立的消息。</p>
<p>首先我们要运行发布消息的脚本，然后在命令中输入要发送的消息的内容：</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">bin/kafka-console-producer.sh --broker-list <span class="number">192.168</span><span class="number">.65</span><span class="number">.60</span>:<span class="number">9092</span> --topic test </span><br><span class="line">&gt;<span class="keyword">this</span> is a msg</span><br><span class="line">&gt;<span class="keyword">this</span> is a another msg </span><br></pre></td></tr></table></figure></li>
<li><p><strong>消费消息</strong></p>
<p><strong>kafka消费消息默认是消费消费者启动之后的消息，也就是说在消费者启动之前的消息不会消费，可以通过配置进行修改（</strong>from-beginning<strong>）</strong></p>
<p>对于consumer，kafka同样也携带了一个命令行客户端，会将获取到内容在命令中进行输出，<strong>默认是消费最新的消息</strong>：</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">bin/kafka-console-consumer.sh --bootstrap-server <span class="number">192.168</span><span class="number">.65</span><span class="number">.60</span>:<span class="number">9092</span> --topic test   </span><br></pre></td></tr></table></figure>

<p>如果想要消费之前的消息可以通过–from-beginning参数指定，如下命令：</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">bin/kafka-console-consumer.sh --bootstrap-server <span class="number">192.168</span><span class="number">.65</span><span class="number">.60</span>:<span class="number">9092</span> --from-beginning --topic test </span><br></pre></td></tr></table></figure>

<p>如果你是通过不同的终端窗口来运行以上的命令，你将会看到在producer终端输入的内容，很快就会在consumer的终端窗口上显示出来。</p>
<p>以上所有的命令都有一些附加的选项；当我们不携带任何参数运行命令的时候，将会显示出这个命令的详细用法。</p>
</li>
<li><p><strong>消费多主题</strong></p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">bin/kafka-console-consumer.sh --bootstrap-server <span class="number">192.168</span><span class="number">.65</span><span class="number">.60</span>:<span class="number">9092</span> --whitelist <span class="string">&quot;test|test-2&quot;</span></span><br></pre></td></tr></table></figure></li>
<li><p><strong>单播消费（rocketmq中的集群消费，同一个消费者组中只有一个消费者可以消费到某个消息）</strong></p>
<p>一条消息只能被某一个消费者消费的模式，类似queue模式，只需让所有消费者在同一个消费组里即可</p>
<p>分别在两个客户端执行如下消费命令，然后往主题里发送消息，结果只有一个客户端能收到消息</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">bin/kafka-console-consumer.sh --bootstrap-server <span class="number">192.168</span><span class="number">.65</span><span class="number">.60</span>:<span class="number">9092</span>  --consumer-property group.id=testGroup --topic test</span><br></pre></td></tr></table></figure></li>
<li><p><strong>多播消费（rocketmq中的广播消费）</strong></p>
<p>一条消息能被多个消费者消费的模式，类似publish-subscribe模式费，针对Kafka同一条消息只能被同一个消费组下的某一个消费者消费的特性，要实现多播只要保证这些消费者属于不同的消费组即可。我们再增加一个消费者，该消费者属于testGroup-2消费组，结果两个客户端都能收到消息</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">bin/kafka-console-consumer.sh --bootstrap-server <span class="number">192.168</span><span class="number">.65</span><span class="number">.60</span>:<span class="number">9092</span> --consumer-property group.id=testGroup-<span class="number">2</span> --topic test </span><br></pre></td></tr></table></figure></li>
<li><p><strong>查看消费组名</strong></p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">bin/kafka-consumer-groups.sh --bootstrap-server <span class="number">192.168</span><span class="number">.65</span><span class="number">.60</span>:<span class="number">9092</span> --list </span><br></pre></td></tr></table></figure></li>
<li><p><strong>查看消费组的消费偏移量</strong></p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">bin/kafka-consumer-groups.sh --bootstrap-server <span class="number">192.168</span><span class="number">.65</span><span class="number">.60</span>:<span class="number">9092</span> --describe --group testGroup</span><br></pre></td></tr></table></figure>

<p><img src="/2022/08/23/03-05-01-Kafka%E5%BF%AB%E9%80%9F%E5%AE%9E%E6%88%98%E4%B8%8E%E5%9F%BA%E6%9C%AC%E5%8E%9F%E7%90%86%E8%AF%A6%E8%A7%A3/image-20220728014609405.png" alt="image-20220728014609405"></p>
<p><strong>current-offset：</strong>当前消费组的已消费偏移量</p>
<p><strong>log-end-offset：</strong>主题对应分区消息的结束偏移量(HW)</p>
<p><strong>lag：</strong>当前消费组未消费的消息数</p>
<p>总结：</p>
<p>对于某个单分区的topic来说，单播消费模式：所有的消费者都属于同一个消费者组；</p>
<p>多播模式：所有的消费者属于不同的几个消费者组，每个消费者组都会消费消息，但是消费者组内部只有一个消费者可以消费消息</p>
</li>
</ul>
<h3 id="4、主题Topic和消息日志Log"><a href="#4、主题Topic和消息日志Log" class="headerlink" title="4、主题Topic和消息日志Log"></a><strong>4、主题Topic和消息日志Log</strong></h3><blockquote>
<p><strong>主题topic是一个逻辑概念，分区时物理概念，实际消息存储在分区上</strong></p>
</blockquote>
<p><strong>可以理解Topic是一个类别的名称，同类消息发送到同一个Topic下面。对于每一个Topic，下面可以有多个分区(Partition)日志文件:</strong></p>
<p><img src="/2022/08/23/03-05-01-Kafka%E5%BF%AB%E9%80%9F%E5%AE%9E%E6%88%98%E4%B8%8E%E5%9F%BA%E6%9C%AC%E5%8E%9F%E7%90%86%E8%AF%A6%E8%A7%A3/image-20220729001332197.png" alt="image-20220729001332197"></p>
<p>Partition是一个<strong>有序的message序列</strong>，这些message按顺序添加到一个叫做<strong>commit log的文件</strong>中。每个partition中的消息都有一个唯一的编号，称之为offset，用来唯一标示某个分区中的message。 </p>
<p><strong>每个partition，都对应一个commit log文件</strong>。一个partition中的message的offset都是唯一的，但是不同的partition中的message的offset可能是相同的。</p>
<p>kafka一般不会删除消息，不管这些消息有没有被消费。只会根据配置的日志保留时间(log.retention.hours，默认168h，一周)确认消息多久被删除，默认保留最近一周的日志消息。kafka的性能与保留的消息数据量大小没有关系，因此保存大量的数据消息日志信息不会有什么影响。</p>
<p><strong>每个consumer是基于自己在commit log中的消费进度(offset)来进行工作的</strong>。在kafka中，<strong>消费offset由consumer自己来维护</strong>；一般情况下我们按照顺序逐条消费commit log中的消息，当然我可以通过指定offset来重复消费某些消息，或者跳过某些消息。</p>
<p>这意味kafka中的consumer对集群的影响是非常小的，添加一个或者减少一个consumer，对于集群或者其他consumer来说，都是没有影响的，因为每个consumer维护各自的消费offset。</p>
<p><strong>多个分区的原因：消息分片存储，存储到不同的机器上，避免单机存储的瓶颈，一个topic下的分区可以在不同的broker上，分布式存储；多个分区，提高消息消费的并行度</strong></p>
<p><strong>创建多个分区的主题：</strong></p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">bin/kafka-topics.sh --create --zookeeper <span class="number">192.168</span><span class="number">.65</span><span class="number">.60</span>:<span class="number">2181</span> --replication-factor <span class="number">1</span> --partitions <span class="number">2</span> --topic test1</span><br></pre></td></tr></table></figure>

<p><strong>查看下topic的情况</strong></p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">bin/kafka-topics.sh --describe --zookeeper <span class="number">192.168</span><span class="number">.65</span><span class="number">.60</span>:<span class="number">2181</span> --topic test1</span><br></pre></td></tr></table></figure>

<p><img src="/2022/08/23/03-05-01-Kafka%E5%BF%AB%E9%80%9F%E5%AE%9E%E6%88%98%E4%B8%8E%E5%9F%BA%E6%9C%AC%E5%8E%9F%E7%90%86%E8%AF%A6%E8%A7%A3/image-20220729001441279.png" alt="image-20220729001441279"></p>
<p>以下是输出内容的解释，第一行是所有分区的概要信息，之后的每一行表示每一个partition的信息。</p>
<ul>
<li>leader节点负责给定partition的所有读写请求。</li>
<li>replicas 表示某个partition在哪几个broker上存在备份。不管这个几点是不是”leader“，甚至这个节点挂了，也会列出。</li>
<li>isr 是replicas的一个子集，它只列出当前还存活着的，并且<strong>已同步备份</strong>了该partition的节点。</li>
</ul>
<p>我们可以运行相同的命令查看之前创建的名称为”test“的topic</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">bin/kafka-topics.sh --describe --zookeeper <span class="number">192.168</span><span class="number">.65</span><span class="number">.60</span>:<span class="number">2181</span> --topic test </span><br></pre></td></tr></table></figure>

<p><img src="/2022/08/23/03-05-01-Kafka%E5%BF%AB%E9%80%9F%E5%AE%9E%E6%88%98%E4%B8%8E%E5%9F%BA%E6%9C%AC%E5%8E%9F%E7%90%86%E8%AF%A6%E8%A7%A3/image-20220729001502295.png" alt="image-20220729001502295"></p>
<p>之前设置了topic的partition数量为1，备份因子为1，因此显示就如上所示了。</p>
<p>可以进入kafka的数据文件存储目录查看test和test1主题的消息日志文件：</p>
<p><img src="/2022/08/23/03-05-01-Kafka%E5%BF%AB%E9%80%9F%E5%AE%9E%E6%88%98%E4%B8%8E%E5%9F%BA%E6%9C%AC%E5%8E%9F%E7%90%86%E8%AF%A6%E8%A7%A3/image-20220729001512351.png" alt="image-20220729001512351"></p>
<p>消息日志文件主要存放在分区文件夹里的以log结尾的日志文件里，如下是test1主题对应的分区0的消息日志：</p>
<p><img src="/2022/08/23/03-05-01-Kafka%E5%BF%AB%E9%80%9F%E5%AE%9E%E6%88%98%E4%B8%8E%E5%9F%BA%E6%9C%AC%E5%8E%9F%E7%90%86%E8%AF%A6%E8%A7%A3/image-20220729001521868.png" alt="image-20220729001521868"></p>
<p>当然我们也可以通过如下命令**增加topic的分区数量(目前kafka不支持减少分区)**：</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">bin/kafka-topics.sh -alter --partitions <span class="number">3</span> --zookeeper <span class="number">192.168</span><span class="number">.65</span><span class="number">.60</span>:<span class="number">2181</span> --topic test     <span class="comment">// 3 表示增加到3个</span></span><br></pre></td></tr></table></figure>

<p>分区之后，消费者会进行rebalance，重新分配</p>
<p><strong>可以这么来理解Topic，Partition和Broker</strong></p>
<p>一个topic，代表逻辑上的一个业务数据集，比如按数据库里不同表的数据操作消息区分放入不同topic，订单相关操作消息放入订单topic，用户相关操作消息放入用户topic，对于大型网站来说，后端数据都是海量的，订单消息很可能是非常巨量的，比如有几百个G甚至达到TB级别，如果把这么多数据都放在一台机器上可定会有容量限制问题，那么就可以在topic内部划分多个partition来分片存储数据，不同的partition可以位于不同的机器上，每台机器上都运行一个Kafka的进程Broker。</p>
<p><strong>为什么要对Topic下数据进行分区存储？</strong></p>
<p>1、commit log文件会受到所在机器的文件系统大小的限制，分区之后可以将不同的分区放在不同的机器上，相当于对数据做了<strong>分布式存储</strong>，理论上一个topic可以处理任意数量的数据。</p>
<p>2、为了<strong>提高并行度</strong>。</p>
<h3 id="5、kafka集群实战"><a href="#5、kafka集群实战" class="headerlink" title="5、kafka集群实战"></a>5、<strong>kafka集群实战</strong></h3><blockquote>
<p>对于kafka来说，一个单独的broker意味着kafka集群中只有一个节点。要想增加kafka集群中的节点数量，只需要多启动几个broker实例即可。为了有更好的理解，现在我们在一台机器上同时启动三个broker实例。</p>
</blockquote>
<p>首先，我们需要建立好其他2个broker的配置文件：</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">cp config/server.properties config/server-<span class="number">1.</span>properties</span><br><span class="line">cp config/server.properties config/server-<span class="number">2.</span>properties</span><br></pre></td></tr></table></figure>

<p>配置文件的需要修改的内容分别如下：</p>
<p>config/server-1.properties:</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">#broker.id属性在kafka集群中必须要是唯一</span><br><span class="line">broker.id=<span class="number">1</span></span><br><span class="line">#kafka部署的机器ip和提供服务的端口号</span><br><span class="line">listeners=PLAINTEXT:<span class="comment">//192.168.65.60:9093   </span></span><br><span class="line">log.dir=/usr/local/data/kafka-logs-<span class="number">1</span></span><br><span class="line">#kafka连接zookeeper的地址，要把多个kafka实例组成集群，对应连接的zookeeper必须相同</span><br><span class="line">zookeeper.connect=<span class="number">192.168</span><span class="number">.65</span><span class="number">.60</span>:<span class="number">2181</span></span><br></pre></td></tr></table></figure>

<p>config/server-2.properties:</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">broker.id=<span class="number">2</span></span><br><span class="line">listeners=PLAINTEXT:<span class="comment">//192.168.65.60:9094</span></span><br><span class="line">log.dir=/usr/local/data/kafka-logs-<span class="number">2</span></span><br><span class="line">zookeeper.connect=<span class="number">192.168</span><span class="number">.65</span><span class="number">.60</span>:<span class="number">2181</span></span><br></pre></td></tr></table></figure>

<p>启动kafka实例：</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">bin/kafka-server-start.sh -daemon config/server-<span class="number">1.</span>properties</span><br><span class="line">bin/kafka-server-start.sh -daemon config/server-<span class="number">2.</span>properties</span><br></pre></td></tr></table></figure>

<p><strong>查看zookeeper确认集群节点是否都注册成功：</strong></p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">ls /brokers/ids</span><br></pre></td></tr></table></figure>

<p>现在我们创建一个新的topic，副本数设置为3，分区数设置为2：</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">bin/kafka-topics.sh --create --zookeeper <span class="number">192.168</span><span class="number">.65</span><span class="number">.60</span>:<span class="number">2181</span> --replication-factor <span class="number">3</span> --partitions <span class="number">2</span> --topic my-replicated-topic</span><br><span class="line"><span class="comment">// 表示创建了一个2个分区，每个分区3个副本 副本的作用时做容灾</span></span><br></pre></td></tr></table></figure>

<p><strong>查看下topic的情况</strong></p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">bin/kafka-topics.sh --describe --zookeeper <span class="number">192.168</span><span class="number">.65</span><span class="number">.60</span>:<span class="number">2181</span> --topic my-replicated-topic</span><br></pre></td></tr></table></figure>

<p><img src="/2022/08/23/03-05-01-Kafka%E5%BF%AB%E9%80%9F%E5%AE%9E%E6%88%98%E4%B8%8E%E5%9F%BA%E6%9C%AC%E5%8E%9F%E7%90%86%E8%AF%A6%E8%A7%A3/image-20220729002031601.png" alt="image-20220729002031601"></p>
<blockquote>
<p>以下是输出内容的解释，第一行是所有分区的概要信息（主题名称，分区数量，副本数量，配置信息），之后的每一行表示每一个partition的信息。</p>
<ul>
<li>leader节点负责给定partition的所有读写请求，同一个主题不同分区leader副本一般不一样(指在不同的broker上，为了容灾)</li>
<li>replicas 表示某个partition在哪几个broker上存在备份。不管这个节点是不是”leader“，甚至这个节点挂了，也会列出。</li>
<li>isr 是replicas的一个子集，它只列出当前还存活着的，并且<strong>已同步备份</strong>了该partition的节点。</li>
</ul>
</blockquote>
<p>现在我们向新建的 my-replicated-topic 中发送一些message，kafka集群可以加上所有kafka节点：</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">bin/kafka-console-producer.sh --broker-list <span class="number">192.168</span><span class="number">.65</span><span class="number">.60</span>:<span class="number">9092</span>,<span class="number">192.168</span><span class="number">.65</span><span class="number">.60</span>:<span class="number">9093</span>,<span class="number">192.168</span><span class="number">.65</span><span class="number">.60</span>:<span class="number">9094</span> --topic my-replicated-topic</span><br><span class="line">&gt;my test msg <span class="number">1</span></span><br><span class="line">&gt;my test msg <span class="number">2</span></span><br></pre></td></tr></table></figure>

<p>现在开始消费：</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">bin/kafka-console-consumer.sh --bootstrap-server <span class="number">192.168</span><span class="number">.65</span><span class="number">.60</span>:<span class="number">9092</span>,<span class="number">192.168</span><span class="number">.65</span><span class="number">.60</span>:<span class="number">9093</span>,<span class="number">192.168</span><span class="number">.65</span><span class="number">.60</span>:<span class="number">9094</span> --from-beginning --topic my-replicated-topic</span><br><span class="line">my test msg <span class="number">1</span></span><br><span class="line">my test msg <span class="number">2</span></span><br></pre></td></tr></table></figure>

<p><strong>总结：</strong></p>
<p><strong>kafka将很多集群关键信息记录在zookeeper里，保证自己的无状态，从而在水平扩容时非常方便。</strong></p>
<h3 id="6、kafka消费模式"><a href="#6、kafka消费模式" class="headerlink" title="6、kafka消费模式"></a>6、kafka消费模式</h3><p><strong>集群消费</strong></p>
<hr>
<p>log的partitions分布在kafka集群中不同的broker上，每个broker可以请求备份其他broker上partition上的数据。kafka集群支持配置一个partition备份的数量。</p>
<p>针对每个partition，都有一个broker起到“leader”的作用，0个或多个其他的broker作为“follwers”的作用。**leader处理所有的针对这个partition的读写请求，而followers被动复制leader的结果，不提供读写(主要是为了保证多副本数据与消费的一致性)**。如果这个leader失效了，其中的一个follower将会自动的变成新的leader。</p>
<p><strong>Producers</strong></p>
<p>生产者将消息发送到topic中去，同时负责选择将message发送到topic的哪一个partition中。通过round-robin做简单的负载均衡。也可以根据消息中的某一个关键字来进行区分。通常第二种方式使用的更多。</p>
<p><strong>Consumers</strong></p>
<p>传统的消息传递模式有2种：队列( queue) 和（publish-subscribe）</p>
<ul>
<li>queue模式：多个consumer从服务器中读取数据，消息只会到达一个consumer。</li>
<li>publish-subscribe模式：消息会被广播给所有的consumer。</li>
</ul>
<p>Kafka基于这2种模式提供了一种consumer的抽象概念：consumer group。</p>
<ul>
<li><p>queue模式：所有的consumer都位于同一个consumer group 下。</p>
</li>
<li><p>publish-subscribe模式：所有的consumer都有着自己唯一的consumer group。</p>
<p><img src="/2022/08/23/03-05-01-Kafka%E5%BF%AB%E9%80%9F%E5%AE%9E%E6%88%98%E4%B8%8E%E5%9F%BA%E6%9C%AC%E5%8E%9F%E7%90%86%E8%AF%A6%E8%A7%A3/image-20220729002258501.png" alt="image-20220729002258501"></p>
</li>
</ul>
<p>上图说明：由2个broker组成的kafka集群，某个主题总共有4个partition(P0-P3)，分别位于不同的broker上。这个集群由2个Consumer Group消费， A有2个consumer instances ，B有4个。</p>
<p>通常一个topic会有几个consumer group，每个consumer group都是一个逻辑上的订阅者（ logical subscriber ）。每个consumer group由多个consumer instance组成，从而达到可扩展和容灾的功能。</p>
<p><strong>消费顺序</strong></p>
<p>一个partition同一个时刻在一个consumer group中只能有一个consumer instance在消费，从而保证消费顺序。</p>
<p><strong>consumer group中的consumer instance的数量不能比一个Topic中的partition的数量多，否则，多出来的consumer消费不到消息。</strong></p>
<p>Kafka只在partition的范围内保证消息消费的局部顺序性，不能在同一个topic中的多个partition中保证总的消费顺序性。</p>
<p>如果有在总体上保证消费顺序的需求，那么我们可以通过将topic的partition数量设置为1，将consumer group中的consumer instance数量也设置为1，但是这样会影响性能，所以kafka的顺序消费很少用。</p>
<h3 id="7、Java客户端访问Kafka"><a href="#7、Java客户端访问Kafka" class="headerlink" title="7、Java客户端访问Kafka"></a>7、<strong>Java客户端访问Kafka</strong></h3><p><strong>maven依赖</strong></p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">&lt;dependency&gt;</span><br><span class="line">   &lt;groupId&gt;org.apache.kafka&lt;/groupId&gt;</span><br><span class="line">   &lt;artifactId&gt;kafka-clients&lt;/artifactId&gt;</span><br><span class="line">   &lt;version&gt;<span class="number">2.4</span><span class="number">.1</span>&lt;/version&gt;</span><br><span class="line">&lt;/dependency&gt;</span><br></pre></td></tr></table></figure>

<p><strong>消息发送端代码</strong></p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> com.tuling.kafka.kafkaDemo;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> com.alibaba.fastjson.JSON;</span><br><span class="line"><span class="keyword">import</span> org.apache.kafka.clients.producer.*;</span><br><span class="line"><span class="keyword">import</span> org.apache.kafka.common.serialization.StringSerializer;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> java.util.Properties;</span><br><span class="line"><span class="keyword">import</span> java.util.concurrent.CountDownLatch;</span><br><span class="line"><span class="keyword">import</span> java.util.concurrent.ExecutionException;</span><br><span class="line"><span class="keyword">import</span> java.util.concurrent.TimeUnit;</span><br><span class="line"></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">MsgProducer</span> </span>&#123;</span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">final</span> <span class="keyword">static</span> String TOPIC_NAME = <span class="string">&quot;my-replicated-topic&quot;</span>;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> <span class="keyword">throws</span> InterruptedException, ExecutionException </span>&#123;</span><br><span class="line">        Properties props = <span class="keyword">new</span> Properties();</span><br><span class="line"><span class="comment">//        props.put(ProducerConfig.BOOTSTRAP_SERVERS_CONFIG, &quot;192.168.31.139:9092,192.168.31.139:9093,192.168.31.139:9094&quot;);</span></span><br><span class="line">        props.put(ProducerConfig.BOOTSTRAP_SERVERS_CONFIG, <span class="string">&quot;192.168.31.139:9092,192.168.31.139:9093,192.168.31.139:9094&quot;</span>);</span><br><span class="line">         <span class="comment">/*</span></span><br><span class="line"><span class="comment">         发出消息持久化机制参数</span></span><br><span class="line"><span class="comment">        （1）acks=0： 表示producer不需要等待任何broker确认收到消息的回复，就可以继续发送下一条消息。性能最高，但是最容易丢消息。</span></span><br><span class="line"><span class="comment">        （2）acks=1： 至少要等待leader已经成功将数据写入本地log，但是不需要等待所有follower是否成功写入。就可以继续发送下一</span></span><br><span class="line"><span class="comment">             条消息。这种情况下，如果follower没有成功备份数据，而此时leader又挂掉，则消息会丢失。</span></span><br><span class="line"><span class="comment">        （3）acks=-1或all： 需要等待 min.insync.replicas(默认为1，推荐配置大于等于2) 这个参数配置的副本个数都成功写入日志，这种策略</span></span><br><span class="line"><span class="comment">            会保证只要有一个备份存活就不会丢失数据。这是最强的数据保证。一般除非是金融级别，或跟钱打交道的场景才会使用这种配置。</span></span><br><span class="line"><span class="comment">         */</span></span><br><span class="line">        props.put(ProducerConfig.ACKS_CONFIG, <span class="string">&quot;1&quot;</span>);</span><br><span class="line">         <span class="comment">/*</span></span><br><span class="line"><span class="comment">        发送失败会重试，默认重试间隔100ms，重试能保证消息发送的可靠性，但是也可能造成消息重复发送，比如网络抖动，所以需要在</span></span><br><span class="line"><span class="comment">        接收者那边做好消息接收的幂等性处理</span></span><br><span class="line"><span class="comment">        */</span></span><br><span class="line">        props.put(ProducerConfig.RETRIES_CONFIG, <span class="number">3</span>);</span><br><span class="line">        <span class="comment">//重试间隔设置</span></span><br><span class="line">        props.put(ProducerConfig.RETRY_BACKOFF_MS_CONFIG, <span class="number">300</span>);</span><br><span class="line">        <span class="comment">//设置发送消息的本地缓冲区，如果设置了该缓冲区，消息会先发送到本地缓冲区，可以提高消息发送性能，默认值是33554432，即32MB</span></span><br><span class="line">        props.put(ProducerConfig.BUFFER_MEMORY_CONFIG, <span class="number">33554432</span>);</span><br><span class="line">        <span class="comment">/*</span></span><br><span class="line"><span class="comment">        kafka本地线程会从缓冲区取数据，批量发送到broker，</span></span><br><span class="line"><span class="comment">        设置批量发送消息的大小，默认值是16384，即16kb，就是说一个batch满了16kb就发送出去</span></span><br><span class="line"><span class="comment">        */</span></span><br><span class="line">        props.put(ProducerConfig.BATCH_SIZE_CONFIG, <span class="number">16384</span>);</span><br><span class="line">        <span class="comment">/*</span></span><br><span class="line"><span class="comment">        默认值是0，意思就是消息必须立即被发送，但这样会影响性能</span></span><br><span class="line"><span class="comment">        一般设置10毫秒左右，就是说这个消息发送完后会进入本地的一个batch，如果10毫秒内，这个batch满了16kb就会随batch一起被发送出去</span></span><br><span class="line"><span class="comment">        如果10毫秒内，batch没满，那么也必须把消息发送出去，不能让消息的发送延迟时间太长</span></span><br><span class="line"><span class="comment">        */</span></span><br><span class="line">        props.put(ProducerConfig.LINGER_MS_CONFIG, <span class="number">10</span>);</span><br><span class="line">        <span class="comment">//把发送的key从字符串序列化为字节数组</span></span><br><span class="line">        props.put(ProducerConfig.KEY_SERIALIZER_CLASS_CONFIG, StringSerializer.class.getName());</span><br><span class="line">        <span class="comment">//把发送消息value从字符串序列化为字节数组</span></span><br><span class="line">        props.put(ProducerConfig.VALUE_SERIALIZER_CLASS_CONFIG, StringSerializer.class.getName());</span><br><span class="line"></span><br><span class="line">        Producer&lt;String, String&gt; producer = <span class="keyword">new</span> KafkaProducer&lt;String, String&gt;(props);</span><br><span class="line"></span><br><span class="line">        <span class="keyword">int</span> msgNum = <span class="number">5</span>;</span><br><span class="line">        <span class="keyword">final</span> CountDownLatch countDownLatch = <span class="keyword">new</span> CountDownLatch(msgNum);</span><br><span class="line">        <span class="keyword">for</span> (<span class="keyword">int</span> i = <span class="number">1</span>; i &lt;= msgNum; i++) &#123;</span><br><span class="line">            Order order = <span class="keyword">new</span> Order(i, <span class="number">100</span> + i, <span class="number">1</span>, <span class="number">1000.00</span>);</span><br><span class="line">            <span class="comment">//指定发送分区</span></span><br><span class="line">            <span class="comment">/*ProducerRecord&lt;String, String&gt; producerRecord = new ProducerRecord&lt;String, String&gt;(TOPIC_NAME</span></span><br><span class="line"><span class="comment">                    , 0, order.getOrderId().toString(), JSON.toJSONString(order));*/</span></span><br><span class="line">            <span class="comment">//未指定发送分区，具体发送的分区计算公式：hash(key)%partitionNum</span></span><br><span class="line">            ProducerRecord&lt;String, String&gt; producerRecord = <span class="keyword">new</span> ProducerRecord&lt;String, String&gt;(TOPIC_NAME</span><br><span class="line">                    , order.getOrderId().toString(), JSON.toJSONString(order));</span><br><span class="line"></span><br><span class="line">            <span class="comment">//等待消息发送成功的同步阻塞方法</span></span><br><span class="line">            RecordMetadata metadata = producer.send(producerRecord).get();</span><br><span class="line">            System.out.println(<span class="string">&quot;同步方式发送消息结果：&quot;</span> + <span class="string">&quot;topic-&quot;</span> + metadata.topic() + <span class="string">&quot;|partition-&quot;</span></span><br><span class="line">                    + metadata.partition() + <span class="string">&quot;|offset-&quot;</span> + metadata.offset());</span><br><span class="line"></span><br><span class="line">            <span class="comment">//异步回调方式发送消息</span></span><br><span class="line">            <span class="comment">/*producer.send(producerRecord, new Callback() &#123;</span></span><br><span class="line"><span class="comment">                public void onCompletion(RecordMetadata metadata, Exception exception) &#123;</span></span><br><span class="line"><span class="comment">                    if (exception != null) &#123;</span></span><br><span class="line"><span class="comment">                        System.err.println(&quot;发送消息失败：&quot; + exception.getStackTrace());</span></span><br><span class="line"><span class="comment"></span></span><br><span class="line"><span class="comment">                    &#125;</span></span><br><span class="line"><span class="comment">                    if (metadata != null) &#123;</span></span><br><span class="line"><span class="comment">                        System.out.println(&quot;异步方式发送消息结果：&quot; + &quot;topic-&quot; + metadata.topic() + &quot;|partition-&quot;</span></span><br><span class="line"><span class="comment">                                + metadata.partition() + &quot;|offset-&quot; + metadata.offset());</span></span><br><span class="line"><span class="comment">                    &#125;</span></span><br><span class="line"><span class="comment">                    countDownLatch.countDown();</span></span><br><span class="line"><span class="comment">                &#125;</span></span><br><span class="line"><span class="comment">            &#125;);*/</span></span><br><span class="line"></span><br><span class="line">            <span class="comment">//送积分 TODO</span></span><br><span class="line"></span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        countDownLatch.await(<span class="number">5</span>, TimeUnit.SECONDS);</span><br><span class="line">        producer.close();</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br></pre></td></tr></table></figure>

<blockquote>
<p>消息发送的关键参数：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">1、ProducerConfig.BOOTSTRAP_SERVERS_CONFIG(bootstrap.servers)  kafka的服务端地址</span><br><span class="line">2、ProducerConfig.ACKS_CONFIG(acks) 发出消息持久化机制参数</span><br><span class="line">（1）acks=0： 表示producer不需要等待任何broker确认收到消息的回复，就可以继续发送下一条消息。性能最高，但是最容易丢消息。</span><br><span class="line">（2）acks=1： 至少要等待leader已经成功将数据写入本地log，但是不需要等待所有follower是否成功写入。就可以继续发送下一</span><br><span class="line">条消息。这种情况下，如果follower没有成功备份数据，而此时leader又挂掉，则消息会丢失。</span><br><span class="line">（3）acks=-1或all： 需要等待 min.insync.replicas(默认为1，推荐配置大于等于2) 这个参数配置的副本个数都成功写入日志，这种策略</span><br><span class="line">会保证只要有一个备份存活就不会丢失数据。这是最强的数据保证。一般除非是金融级别，或跟钱打交道的场景才会使用这种配置。</span><br><span class="line">3、ProducerConfig.RETRIES_CONFIG(retries)重试参数，客户端发送消息没成功，kafka会默认帮忙重试。比如客户端发送消息到kafka服务端，kafka响应客户端ack的时候出现了网络分区，那么客户端没收到ack响应，会重新发送消息</span><br><span class="line">4、ProducerConfig.BUFFER_MEMORY_CONFIG（buffer.memory）本地缓冲区，生产者发送消息会先发送到缓冲区中，然后会有一个线程消费缓冲区数据</span><br></pre></td></tr></table></figure>
</blockquote>
<p><strong>消息消费端</strong></p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> com.tuling.kafka.kafkaDemo;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> org.apache.kafka.clients.consumer.ConsumerConfig;</span><br><span class="line"><span class="keyword">import</span> org.apache.kafka.clients.consumer.ConsumerRecord;</span><br><span class="line"><span class="keyword">import</span> org.apache.kafka.clients.consumer.ConsumerRecords;</span><br><span class="line"><span class="keyword">import</span> org.apache.kafka.clients.consumer.KafkaConsumer;</span><br><span class="line"><span class="keyword">import</span> org.apache.kafka.common.serialization.StringDeserializer;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> java.time.Duration;</span><br><span class="line"><span class="keyword">import</span> java.util.Arrays;</span><br><span class="line"><span class="keyword">import</span> java.util.Properties;</span><br><span class="line"></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">MsgConsumer</span> </span>&#123;</span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">final</span> <span class="keyword">static</span> String TOPIC_NAME = <span class="string">&quot;my-replicated-topic&quot;</span>;</span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">final</span> <span class="keyword">static</span> String CONSUMER_GROUP_NAME = <span class="string">&quot;testGroup&quot;</span>;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">        Properties props = <span class="keyword">new</span> Properties();</span><br><span class="line">        props.put(ConsumerConfig.BOOTSTRAP_SERVERS_CONFIG, <span class="string">&quot;192.168.65.60:9092,192.168.65.60:9093,192.168.65.60:9094&quot;</span>);</span><br><span class="line">        <span class="comment">// 消费分组名</span></span><br><span class="line">        props.put(ConsumerConfig.GROUP_ID_CONFIG, CONSUMER_GROUP_NAME);</span><br><span class="line">        <span class="comment">// 是否自动提交offset，默认就是true</span></span><br><span class="line">        <span class="comment">/*props.put(ConsumerConfig.ENABLE_AUTO_COMMIT_CONFIG, &quot;true&quot;);</span></span><br><span class="line"><span class="comment">        // 自动提交offset的间隔时间</span></span><br><span class="line"><span class="comment">        props.put(ConsumerConfig.AUTO_COMMIT_INTERVAL_MS_CONFIG, &quot;1000&quot;);*/</span></span><br><span class="line">        props.put(ConsumerConfig.ENABLE_AUTO_COMMIT_CONFIG, <span class="string">&quot;false&quot;</span>);</span><br><span class="line">        <span class="comment">/*</span></span><br><span class="line"><span class="comment">        当消费主题的是一个新的消费组，或者指定offset的消费方式，offset不存在，那么应该如何消费</span></span><br><span class="line"><span class="comment">        latest(默认) ：只消费自己启动之后发送到主题的消息</span></span><br><span class="line"><span class="comment">        earliest：第一次从头开始消费，以后按照消费offset记录继续消费，这个需要区别于consumer.seekToBeginning(每次都从头开始消费)</span></span><br><span class="line"><span class="comment">        */</span></span><br><span class="line">        <span class="comment">//props.put(ConsumerConfig.AUTO_OFFSET_RESET_CONFIG, &quot;earliest&quot;);</span></span><br><span class="line">		<span class="comment">/*</span></span><br><span class="line"><span class="comment">		consumer给broker发送心跳的间隔时间，broker接收到心跳如果此时有rebalance发生会通过心跳响应将</span></span><br><span class="line"><span class="comment">		rebalance方案下发给consumer，这个时间可以稍微短一点</span></span><br><span class="line"><span class="comment">		*/</span></span><br><span class="line">        props.put(ConsumerConfig.HEARTBEAT_INTERVAL_MS_CONFIG, <span class="number">1000</span>);</span><br><span class="line">        <span class="comment">/*</span></span><br><span class="line"><span class="comment">        服务端broker多久感知不到一个consumer心跳就认为他故障了，会将其踢出消费组，</span></span><br><span class="line"><span class="comment">        对应的Partition也会被重新分配给其他consumer，默认是10秒</span></span><br><span class="line"><span class="comment">        */</span></span><br><span class="line">        props.put(ConsumerConfig.SESSION_TIMEOUT_MS_CONFIG, <span class="number">10</span> * <span class="number">1000</span>);</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">        <span class="comment">//一次poll最大拉取消息的条数，如果消费者处理速度很快，可以设置大点，如果处理速度一般，可以设置小点</span></span><br><span class="line">        props.put(ConsumerConfig.MAX_POLL_RECORDS_CONFIG, <span class="number">50</span>);</span><br><span class="line">        <span class="comment">/*</span></span><br><span class="line"><span class="comment">        如果两次poll操作间隔超过了这个时间，broker就会认为这个consumer处理能力太弱，</span></span><br><span class="line"><span class="comment">        会将其踢出消费组，将分区分配给别的consumer消费</span></span><br><span class="line"><span class="comment">        */</span></span><br><span class="line">        props.put(ConsumerConfig.MAX_POLL_INTERVAL_MS_CONFIG, <span class="number">30</span> * <span class="number">1000</span>);</span><br><span class="line"></span><br><span class="line">        props.put(ConsumerConfig.KEY_DESERIALIZER_CLASS_CONFIG, StringDeserializer.class.getName());</span><br><span class="line">        props.put(ConsumerConfig.VALUE_DESERIALIZER_CLASS_CONFIG, StringDeserializer.class.getName());</span><br><span class="line">        KafkaConsumer&lt;String, String&gt; consumer = <span class="keyword">new</span> KafkaConsumer&lt;String, String&gt;(props);</span><br><span class="line"></span><br><span class="line">        consumer.subscribe(Arrays.asList(TOPIC_NAME));</span><br><span class="line">        <span class="comment">// 消费指定分区</span></span><br><span class="line">        <span class="comment">//consumer.assign(Arrays.asList(new TopicPartition(TOPIC_NAME, 0)));</span></span><br><span class="line"></span><br><span class="line">        <span class="comment">//消息回溯消费  消息从头消费</span></span><br><span class="line">        <span class="comment">/*consumer.assign(Arrays.asList(new TopicPartition(TOPIC_NAME, 0)));</span></span><br><span class="line"><span class="comment">        consumer.seekToBeginning(Arrays.asList(new TopicPartition(TOPIC_NAME, 0)));*/</span></span><br><span class="line"></span><br><span class="line">        <span class="comment">//指定offset消费</span></span><br><span class="line">        <span class="comment">/*consumer.assign(Arrays.asList(new TopicPartition(TOPIC_NAME, 0)));</span></span><br><span class="line"><span class="comment">        consumer.seek(new TopicPartition(TOPIC_NAME, 0), 10);*/</span></span><br><span class="line"></span><br><span class="line">        <span class="comment">//从指定时间点开始消费</span></span><br><span class="line"></span><br><span class="line">        <span class="comment">/*List&lt;PartitionInfo&gt; topicPartitions = consumer.partitionsFor(TOPIC_NAME);</span></span><br><span class="line"><span class="comment">        //从1小时前开始消费</span></span><br><span class="line"><span class="comment">        long fetchDataTime = new Date().getTime() - 1000 * 60 * 60;</span></span><br><span class="line"><span class="comment">        Map&lt;TopicPartition, Long&gt; map = new HashMap&lt;&gt;();</span></span><br><span class="line"><span class="comment">        for (PartitionInfo par : topicPartitions) &#123;</span></span><br><span class="line"><span class="comment">            map.put(new TopicPartition(TOPIC_NAME, par.partition()), fetchDataTime);</span></span><br><span class="line"><span class="comment">        &#125;</span></span><br><span class="line"><span class="comment">        Map&lt;TopicPartition, OffsetAndTimestamp&gt; parMap = consumer.offsetsForTimes(map);</span></span><br><span class="line"><span class="comment">        for (Map.Entry&lt;TopicPartition, OffsetAndTimestamp&gt; entry : parMap.entrySet()) &#123;</span></span><br><span class="line"><span class="comment">            TopicPartition key = entry.getKey();</span></span><br><span class="line"><span class="comment">            OffsetAndTimestamp value = entry.getValue();</span></span><br><span class="line"><span class="comment">            if (key == null || value == null) continue;</span></span><br><span class="line"><span class="comment">            Long offset = value.offset();</span></span><br><span class="line"><span class="comment">            System.out.println(&quot;partition-&quot; + key.partition() + &quot;|offset-&quot; + offset);</span></span><br><span class="line"><span class="comment">            System.out.println();</span></span><br><span class="line"><span class="comment">            //根据消费里的timestamp确定offset</span></span><br><span class="line"><span class="comment">            if (value != null) &#123;</span></span><br><span class="line"><span class="comment">                consumer.assign(Arrays.asList(key));</span></span><br><span class="line"><span class="comment">                consumer.seek(key, offset);</span></span><br><span class="line"><span class="comment">            &#125;</span></span><br><span class="line"><span class="comment">        &#125;*/</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">        <span class="keyword">while</span> (<span class="keyword">true</span>) &#123;</span><br><span class="line">            <span class="comment">/*</span></span><br><span class="line"><span class="comment">             * poll() API 是拉取消息的长轮询</span></span><br><span class="line"><span class="comment">             */</span></span><br><span class="line">            ConsumerRecords&lt;String, String&gt; records = consumer.poll(Duration.ofMillis(<span class="number">1000</span>));</span><br><span class="line">            <span class="keyword">for</span> (ConsumerRecord&lt;String, String&gt; record : records) &#123;</span><br><span class="line">                System.out.printf(<span class="string">&quot;收到消息：partition = %d,offset = %d, key = %s, value = %s%n&quot;</span>, record.partition(),</span><br><span class="line">                        record.offset(), record.key(), record.value());</span><br><span class="line">            &#125;</span><br><span class="line"></span><br><span class="line">            <span class="keyword">if</span> (records.count() &gt; <span class="number">0</span>) &#123;</span><br><span class="line">                <span class="comment">// 手动同步提交offset，当前线程会阻塞直到offset提交成功</span></span><br><span class="line">                <span class="comment">// 一般使用同步提交，因为提交之后一般也没有什么逻辑代码了</span></span><br><span class="line">                <span class="comment">//consumer.commitSync();</span></span><br><span class="line"></span><br><span class="line">                <span class="comment">// 手动异步提交offset，当前线程提交offset不会阻塞，可以继续处理后面的程序逻辑</span></span><br><span class="line">                <span class="comment">/*consumer.commitAsync(new OffsetCommitCallback() &#123;</span></span><br><span class="line"><span class="comment">                    @Override</span></span><br><span class="line"><span class="comment">                    public void onComplete(Map&lt;TopicPartition, OffsetAndMetadata&gt; offsets, Exception exception) &#123;</span></span><br><span class="line"><span class="comment">                        if (exception != null) &#123;</span></span><br><span class="line"><span class="comment">                            System.err.println(&quot;Commit failed for &quot; + offsets);</span></span><br><span class="line"><span class="comment">                            System.err.println(&quot;Commit failed exception: &quot; + exception.getStackTrace());</span></span><br><span class="line"><span class="comment">                        &#125;</span></span><br><span class="line"><span class="comment">                    &#125;</span></span><br><span class="line"><span class="comment">                &#125;);*/</span></span><br><span class="line"></span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br></pre></td></tr></table></figure>

<h3 id="8、Spring-Boot整合Kafka"><a href="#8、Spring-Boot整合Kafka" class="headerlink" title="8、Spring Boot整合Kafka"></a>8、<strong>Spring Boot整合Kafka</strong></h3><p><strong>maven坐标</strong></p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">&lt;dependency&gt;</span><br><span class="line">    &lt;groupId&gt;org.springframework.kafka&lt;/groupId&gt;</span><br><span class="line">    &lt;artifactId&gt;spring-kafka&lt;/artifactId&gt;</span><br><span class="line">&lt;/dependency&gt;</span><br></pre></td></tr></table></figure>

<p><strong>application.yml配置如下</strong></p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br></pre></td><td class="code"><pre><span class="line">server:</span><br><span class="line">  port: <span class="number">8080</span></span><br><span class="line"></span><br><span class="line">spring:</span><br><span class="line">  kafka:</span><br><span class="line">    bootstrap-servers: <span class="number">192.168</span><span class="number">.65</span><span class="number">.60</span>:<span class="number">9092</span>,<span class="number">192.168</span><span class="number">.65</span><span class="number">.60</span>:<span class="number">9093</span>,<span class="number">192.168</span><span class="number">.65</span><span class="number">.60</span>:<span class="number">9094</span></span><br><span class="line">    producer: # 生产者</span><br><span class="line">      retries: <span class="number">3</span> # 设置大于<span class="number">0</span>的值，则客户端会将发送失败的记录重新发送</span><br><span class="line">      batch-size: <span class="number">16384</span></span><br><span class="line">      buffer-memory: <span class="number">33554432</span></span><br><span class="line">      acks: <span class="number">1</span></span><br><span class="line">      # 指定消息key和消息体的编解码方式</span><br><span class="line">      key-serializer: org.apache.kafka.common.serialization.StringSerializer</span><br><span class="line">      value-serializer: org.apache.kafka.common.serialization.StringSerializer</span><br><span class="line">    consumer:</span><br><span class="line">      group-id: <span class="keyword">default</span>-group</span><br><span class="line">      enable-auto-commit: <span class="keyword">false</span></span><br><span class="line">      auto-offset-reset: earliest</span><br><span class="line">      key-deserializer: org.apache.kafka.common.serialization.StringDeserializer</span><br><span class="line">      value-deserializer: org.apache.kafka.common.serialization.StringDeserializer</span><br><span class="line">    listener:</span><br><span class="line">      # 当每一条记录被消费者监听器（ListenerConsumer）处理之后提交</span><br><span class="line">      # RECORD</span><br><span class="line">      # 当每一批poll()的数据被消费者监听器（ListenerConsumer）处理之后提交</span><br><span class="line">      # BATCH</span><br><span class="line">      # 当每一批poll()的数据被消费者监听器（ListenerConsumer）处理之后，距离上次提交时间大于TIME时提交</span><br><span class="line">      # TIME</span><br><span class="line">      # 当每一批poll()的数据被消费者监听器（ListenerConsumer）处理之后，被处理record数量大于等于COUNT时提交</span><br><span class="line">      # COUNT</span><br><span class="line">      # TIME |　COUNT　有一个条件满足时提交</span><br><span class="line">      # COUNT_TIME</span><br><span class="line">      # 当每一批poll()的数据被消费者监听器（ListenerConsumer）处理之后, 手动调用Acknowledgment.acknowledge()后提交</span><br><span class="line">      # MANUAL</span><br><span class="line">      # 手动调用Acknowledgment.acknowledge()后立即提交，一般使用这种</span><br><span class="line">      # MANUAL_IMMEDIATE</span><br><span class="line">      ack-mode: manual_immediate</span><br></pre></td></tr></table></figure>

<p><strong>发送者代码</strong></p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> com.kafka;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> org.springframework.beans.factory.annotation.Autowired;</span><br><span class="line"><span class="keyword">import</span> org.springframework.kafka.core.KafkaTemplate;</span><br><span class="line"><span class="keyword">import</span> org.springframework.web.bind.annotation.RequestMapping;</span><br><span class="line"><span class="keyword">import</span> org.springframework.web.bind.annotation.RestController;</span><br><span class="line"></span><br><span class="line"><span class="meta">@RestController</span></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">KafkaController</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">final</span> <span class="keyword">static</span> String TOPIC_NAME = <span class="string">&quot;my-replicated-topic&quot;</span>;</span><br><span class="line"></span><br><span class="line">    <span class="meta">@Autowired</span></span><br><span class="line">    <span class="keyword">private</span> KafkaTemplate&lt;String, String&gt; kafkaTemplate;</span><br><span class="line"></span><br><span class="line">    <span class="meta">@RequestMapping(&quot;/send&quot;)</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">send</span><span class="params">()</span> </span>&#123;</span><br><span class="line">        kafkaTemplate.send(TOPIC_NAME, <span class="number">0</span>, <span class="string">&quot;key&quot;</span>, <span class="string">&quot;this is a msg&quot;</span>);</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p><strong>消费者代码</strong></p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> com.kafka;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> org.apache.kafka.clients.consumer.ConsumerRecord;</span><br><span class="line"><span class="keyword">import</span> org.springframework.kafka.annotation.KafkaListener;</span><br><span class="line"><span class="keyword">import</span> org.springframework.kafka.support.Acknowledgment;</span><br><span class="line"><span class="keyword">import</span> org.springframework.stereotype.Component;</span><br><span class="line"></span><br><span class="line"><span class="meta">@Component</span></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">MyConsumer</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">    <span class="comment">/**</span></span><br><span class="line"><span class="comment">     * <span class="doctag">@KafkaListener</span>(groupId = &quot;testGroup&quot;, topicPartitions = &#123;</span></span><br><span class="line"><span class="comment">     *             <span class="doctag">@TopicPartition</span>(topic = &quot;topic1&quot;, partitions = &#123;&quot;0&quot;, &quot;1&quot;&#125;),</span></span><br><span class="line"><span class="comment">     *             <span class="doctag">@TopicPartition</span>(topic = &quot;topic2&quot;, partitions = &quot;0&quot;,</span></span><br><span class="line"><span class="comment">     *                     partitionOffsets = <span class="doctag">@PartitionOffset</span>(partition = &quot;1&quot;, initialOffset = &quot;100&quot;))</span></span><br><span class="line"><span class="comment">     *     &#125;,concurrency = &quot;6&quot;)</span></span><br><span class="line"><span class="comment">     *  //concurrency就是同组下的消费者个数，就是并发消费数，必须小于等于分区总数</span></span><br><span class="line"><span class="comment">     * <span class="doctag">@param</span> record</span></span><br><span class="line"><span class="comment">     */</span></span><br><span class="line">    <span class="meta">@KafkaListener(topics = &quot;my-replicated-topic&quot;,groupId = &quot;zhugeGroup&quot;)</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">listenZhugeGroup</span><span class="params">(ConsumerRecord&lt;String, String&gt; record, Acknowledgment ack)</span> </span>&#123;</span><br><span class="line">        String value = record.value();</span><br><span class="line">        System.out.println(value);</span><br><span class="line">        System.out.println(record);</span><br><span class="line">        <span class="comment">//手动提交offset</span></span><br><span class="line">        ack.acknowledge();</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="comment">/*//配置多个消费组</span></span><br><span class="line"><span class="comment">    @KafkaListener(topics = &quot;my-replicated-topic&quot;,groupId = &quot;tulingGroup&quot;)</span></span><br><span class="line"><span class="comment">    public void listenTulingGroup(ConsumerRecord&lt;String, String&gt; record, Acknowledgment ack) &#123;</span></span><br><span class="line"><span class="comment">        String value = record.value();</span></span><br><span class="line"><span class="comment">        System.out.println(value);</span></span><br><span class="line"><span class="comment">        System.out.println(record);</span></span><br><span class="line"><span class="comment">        ack.acknowledge();</span></span><br><span class="line"><span class="comment">    &#125;*/</span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>



<p><a target="_blank" rel="noopener" href="https://www.processon.com/view/link/62e7be845653bb07161a4b37">kafka思维导图</a></p>
 
      <!-- reward -->
      
    </div>
    

    <!-- copyright -->
    
    <div class="declare">
      <ul class="post-copyright">
        <li>
          <i class="ri-copyright-line"></i>
          <strong>Copyright： </strong>
          
          Copyright is owned by the author. For commercial reprints, please contact the author for authorization. For non-commercial reprints, please indicate the source.
          
        </li>
      </ul>
    </div>
    
    <footer class="article-footer">
       
<div class="share-btn">
      <span class="share-sns share-outer">
        <i class="ri-share-forward-line"></i>
        分享
      </span>
      <div class="share-wrap">
        <i class="arrow"></i>
        <div class="share-icons">
          
          <a class="weibo share-sns" href="javascript:;" data-type="weibo">
            <i class="ri-weibo-fill"></i>
          </a>
          <a class="weixin share-sns wxFab" href="javascript:;" data-type="weixin">
            <i class="ri-wechat-fill"></i>
          </a>
          <a class="qq share-sns" href="javascript:;" data-type="qq">
            <i class="ri-qq-fill"></i>
          </a>
          <a class="douban share-sns" href="javascript:;" data-type="douban">
            <i class="ri-douban-line"></i>
          </a>
          <!-- <a class="qzone share-sns" href="javascript:;" data-type="qzone">
            <i class="icon icon-qzone"></i>
          </a> -->
          
          <a class="facebook share-sns" href="javascript:;" data-type="facebook">
            <i class="ri-facebook-circle-fill"></i>
          </a>
          <a class="twitter share-sns" href="javascript:;" data-type="twitter">
            <i class="ri-twitter-fill"></i>
          </a>
          <a class="google share-sns" href="javascript:;" data-type="google">
            <i class="ri-google-fill"></i>
          </a>
        </div>
      </div>
</div>

<div class="wx-share-modal">
    <a class="modal-close" href="javascript:;"><i class="ri-close-circle-line"></i></a>
    <p>扫一扫，分享到微信</p>
    <div class="wx-qrcode">
      <img src="//api.qrserver.com/v1/create-qr-code/?size=150x150&data=https://haze0x.github.io/2022/08/23/03-05-01-Kafka%E5%BF%AB%E9%80%9F%E5%AE%9E%E6%88%98%E4%B8%8E%E5%9F%BA%E6%9C%AC%E5%8E%9F%E7%90%86%E8%AF%A6%E8%A7%A3/" alt="微信分享二维码">
    </div>
</div>

<div id="share-mask"></div>  
  <ul class="article-tag-list" itemprop="keywords"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/kafka/" rel="tag">kafka</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/%E5%88%86%E5%B8%83%E5%BC%8F/" rel="tag">分布式</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/%E6%B6%88%E6%81%AF%E9%98%9F%E5%88%97/" rel="tag">消息队列</a></li></ul>

    </footer>
  </div>

   
  <nav class="article-nav">
    
    
      <a href="/2022/08/22/03-04-06-RocketMQ%E5%AE%9E%E8%B7%B5%E9%97%AE%E9%A2%98%E6%80%BB%E7%BB%93/" class="article-nav-link">
        <strong class="article-nav-caption">下一篇</strong>
        <div class="article-nav-title">RocketMQ实践问题总结</div>
      </a>
    
  </nav>

   
<!-- valine评论 -->
<div id="vcomments-box">
  <div id="vcomments"></div>
</div>
<script src="//cdn1.lncld.net/static/js/3.0.4/av-min.js"></script>
<script src="https://cdn.jsdelivr.net/npm/valine@1.4.14/dist/Valine.min.js"></script>
<script>
  new Valine({
    el: "#vcomments",
    app_id: "z7pjLOiS1FEJ17pkhGOjBQm4-gzGzoHsz",
    app_key: "Ytw1ShVWHkcjtz9lybfPBsVT",
    path: window.location.pathname,
    avatar: "monsterid",
    placeholder: "给我的文章加点评论吧~",
    recordIP: true,
  });
  const infoEle = document.querySelector("#vcomments .info");
  if (infoEle && infoEle.childNodes && infoEle.childNodes.length > 0) {
    infoEle.childNodes.forEach(function (item) {
      item.parentNode.removeChild(item);
    });
  }
</script>
<style>
  #vcomments-box {
    padding: 5px 30px;
  }

  @media screen and (max-width: 800px) {
    #vcomments-box {
      padding: 5px 0px;
    }
  }

  #vcomments-box #vcomments {
    background-color: #fff;
  }

  .v .vlist .vcard .vh {
    padding-right: 20px;
  }

  .v .vlist .vcard {
    padding-left: 10px;
  }
</style>

 
   
     
</article>

</section>
      <footer class="footer">
  <div class="outer">
    <ul>
      <li>
        Copyrights &copy;
        2015-2022
        <i class="ri-heart-fill heart_icon"></i> John Doe
      </li>
    </ul>
    <ul>
      <li>
        
      </li>
    </ul>
    <ul>
      <li>
        
        
        <span>
  <span><i class="ri-user-3-fill"></i>Visitors:<span id="busuanzi_value_site_uv"></span></span>
  <span class="division">|</span>
  <span><i class="ri-eye-fill"></i>Views:<span id="busuanzi_value_page_pv"></span></span>
</span>
        
      </li>
    </ul>
    <ul>
      
    </ul>
    <ul>
      
    </ul>
    <ul>
      <li>
        <!-- cnzz统计 -->
        
        <script type="text/javascript" src='https://s9.cnzz.com/z_stat.php?id=1278069914&amp;web_id=1278069914'></script>
        
      </li>
    </ul>
  </div>
</footer>    
    </main>
    <div class="float_btns">
      <div class="totop" id="totop">
  <i class="ri-arrow-up-line"></i>
</div>

<div class="todark" id="todark">
  <i class="ri-moon-line"></i>
</div>

    </div>
    <aside class="sidebar on">
      <button class="navbar-toggle"></button>
<nav class="navbar">
  
  <div class="logo">
    <a href="/"><img src="/images/ayer-side.svg" alt="Sword Heart"></a>
  </div>
  
  <ul class="nav nav-main">
    
    <li class="nav-item">
      <a class="nav-item-link" href="/">主页</a>
    </li>
    
    <li class="nav-item">
      <a class="nav-item-link" href="/archives">归档</a>
    </li>
    
    <li class="nav-item">
      <a class="nav-item-link" href="/categories">分类</a>
    </li>
    
    <li class="nav-item">
      <a class="nav-item-link" href="/tags">标签</a>
    </li>
    
    <li class="nav-item">
      <a class="nav-item-link" href="/tags/Jvm">性能调优</a>
    </li>
    
    <li class="nav-item">
      <a class="nav-item-link" href="/tags/%E5%B9%B6%E5%8F%91%E7%BC%96%E7%A8%8B">并发编程</a>
    </li>
    
    <li class="nav-item">
      <a class="nav-item-link" href="/tags/%E6%BA%90%E7%A0%81%E6%A1%86%E6%9E%B6">源码框架</a>
    </li>
    
    <li class="nav-item">
      <a class="nav-item-link" href="/tags/%E5%88%86%E5%B8%83%E5%BC%8F">分布式</a>
    </li>
    
    <li class="nav-item">
      <a class="nav-item-link" href="/tags/%E7%AE%97%E6%B3%95">算法</a>
    </li>
    
    <li class="nav-item">
      <a class="nav-item-link" href="/2021/12/11/about">关于我</a>
    </li>
    
    <li class="nav-item">
      <a class="nav-item-link" href="/player">播放器</a>
    </li>
    
  </ul>
</nav>
<nav class="navbar navbar-bottom">
  <ul class="nav">
    <li class="nav-item">
      
      <a class="nav-item-link nav-item-search"  title="Search">
        <i class="ri-search-line"></i>
      </a>
      
      
      <a class="nav-item-link" target="_blank" href="/atom.xml" title="RSS Feed">
        <i class="ri-rss-line"></i>
      </a>
      
    </li>
  </ul>
</nav>
<div class="search-form-wrap">
  <div class="local-search local-search-plugin">
  <input type="search" id="local-search-input" class="local-search-input" placeholder="Search...">
  <div id="local-search-result" class="local-search-result"></div>
</div>
</div>
    </aside>
    <div id="mask"></div>

<!-- #reward -->
<div id="reward">
  <span class="close"><i class="ri-close-line"></i></span>
  <p class="reward-p"><i class="ri-cup-line"></i>请我喝杯咖啡吧~</p>
  <div class="reward-box">
    
    <div class="reward-item">
      <img class="reward-img" src="https://cdn.jsdelivr.net/gh/Shen-Yu/cdn/img/alipay.jpg">
      <span class="reward-type">支付宝</span>
    </div>
    
    
    <div class="reward-item">
      <img class="reward-img" src="https://cdn.jsdelivr.net/gh/Shen-Yu/cdn/img/wechat.jpg">
      <span class="reward-type">微信</span>
    </div>
    
  </div>
</div>
    
<script src="/js/jquery-3.6.0.min.js"></script>
 
<script src="/js/lazyload.min.js"></script>

<!-- Tocbot -->
 
<script src="/js/tocbot.min.js"></script>

<script>
  tocbot.init({
    tocSelector: ".tocbot",
    contentSelector: ".article-entry",
    headingSelector: "h1, h2, h3, h4, h5, h6",
    hasInnerContainers: true,
    scrollSmooth: true,
    scrollContainer: "main",
    positionFixedSelector: ".tocbot",
    positionFixedClass: "is-position-fixed",
    fixedSidebarOffset: "auto",
  });
</script>

<script src="https://cdn.jsdelivr.net/npm/jquery-modal@0.9.2/jquery.modal.min.js"></script>
<link
  rel="stylesheet"
  href="https://cdn.jsdelivr.net/npm/jquery-modal@0.9.2/jquery.modal.min.css"
/>
<script src="https://cdn.jsdelivr.net/npm/justifiedGallery@3.7.0/dist/js/jquery.justifiedGallery.min.js"></script>

<script src="/dist/main.js"></script>

<!-- ImageViewer -->
 <!-- Root element of PhotoSwipe. Must have class pswp. -->
<div class="pswp" tabindex="-1" role="dialog" aria-hidden="true">

    <!-- Background of PhotoSwipe. 
         It's a separate element as animating opacity is faster than rgba(). -->
    <div class="pswp__bg"></div>

    <!-- Slides wrapper with overflow:hidden. -->
    <div class="pswp__scroll-wrap">

        <!-- Container that holds slides. 
            PhotoSwipe keeps only 3 of them in the DOM to save memory.
            Don't modify these 3 pswp__item elements, data is added later on. -->
        <div class="pswp__container">
            <div class="pswp__item"></div>
            <div class="pswp__item"></div>
            <div class="pswp__item"></div>
        </div>

        <!-- Default (PhotoSwipeUI_Default) interface on top of sliding area. Can be changed. -->
        <div class="pswp__ui pswp__ui--hidden">

            <div class="pswp__top-bar">

                <!--  Controls are self-explanatory. Order can be changed. -->

                <div class="pswp__counter"></div>

                <button class="pswp__button pswp__button--close" title="Close (Esc)"></button>

                <button class="pswp__button pswp__button--share" style="display:none" title="Share"></button>

                <button class="pswp__button pswp__button--fs" title="Toggle fullscreen"></button>

                <button class="pswp__button pswp__button--zoom" title="Zoom in/out"></button>

                <!-- Preloader demo http://codepen.io/dimsemenov/pen/yyBWoR -->
                <!-- element will get class pswp__preloader--active when preloader is running -->
                <div class="pswp__preloader">
                    <div class="pswp__preloader__icn">
                        <div class="pswp__preloader__cut">
                            <div class="pswp__preloader__donut"></div>
                        </div>
                    </div>
                </div>
            </div>

            <div class="pswp__share-modal pswp__share-modal--hidden pswp__single-tap">
                <div class="pswp__share-tooltip"></div>
            </div>

            <button class="pswp__button pswp__button--arrow--left" title="Previous (arrow left)">
            </button>

            <button class="pswp__button pswp__button--arrow--right" title="Next (arrow right)">
            </button>

            <div class="pswp__caption">
                <div class="pswp__caption__center"></div>
            </div>

        </div>

    </div>

</div>

<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/photoswipe@4.1.3/dist/photoswipe.min.css">
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/photoswipe@4.1.3/dist/default-skin/default-skin.min.css">
<script src="https://cdn.jsdelivr.net/npm/photoswipe@4.1.3/dist/photoswipe.min.js"></script>
<script src="https://cdn.jsdelivr.net/npm/photoswipe@4.1.3/dist/photoswipe-ui-default.min.js"></script>

<script>
    function viewer_init() {
        let pswpElement = document.querySelectorAll('.pswp')[0];
        let $imgArr = document.querySelectorAll(('.article-entry img:not(.reward-img)'))

        $imgArr.forEach(($em, i) => {
            $em.onclick = () => {
                // slider展开状态
                // todo: 这样不好，后面改成状态
                if (document.querySelector('.left-col.show')) return
                let items = []
                $imgArr.forEach(($em2, i2) => {
                    let img = $em2.getAttribute('data-idx', i2)
                    let src = $em2.getAttribute('data-target') || $em2.getAttribute('src')
                    let title = $em2.getAttribute('alt')
                    // 获得原图尺寸
                    const image = new Image()
                    image.src = src
                    items.push({
                        src: src,
                        w: image.width || $em2.width,
                        h: image.height || $em2.height,
                        title: title
                    })
                })
                var gallery = new PhotoSwipe(pswpElement, PhotoSwipeUI_Default, items, {
                    index: parseInt(i)
                });
                gallery.init()
            }
        })
    }
    viewer_init()
</script> 
<!-- MathJax -->

<!-- Katex -->

<!-- busuanzi  -->
 
<script src="/js/busuanzi-2.3.pure.min.js"></script>
 
<!-- ClickLove -->

<!-- ClickBoom1 -->

<!-- ClickBoom2 -->
 
<script src="/js/clickBoom2.js"></script>
 
<!-- CodeCopy -->
 
<link rel="stylesheet" href="/css/clipboard.css">
 <script src="https://cdn.jsdelivr.net/npm/clipboard@2/dist/clipboard.min.js"></script>
<script>
  function wait(callback, seconds) {
    var timelag = null;
    timelag = window.setTimeout(callback, seconds);
  }
  !function (e, t, a) {
    var initCopyCode = function(){
      var copyHtml = '';
      copyHtml += '<button class="btn-copy" data-clipboard-snippet="">';
      copyHtml += '<i class="ri-file-copy-2-line"></i><span>COPY</span>';
      copyHtml += '</button>';
      $(".highlight .code pre").before(copyHtml);
      $(".article pre code").before(copyHtml);
      var clipboard = new ClipboardJS('.btn-copy', {
        target: function(trigger) {
          return trigger.nextElementSibling;
        }
      });
      clipboard.on('success', function(e) {
        let $btn = $(e.trigger);
        $btn.addClass('copied');
        let $icon = $($btn.find('i'));
        $icon.removeClass('ri-file-copy-2-line');
        $icon.addClass('ri-checkbox-circle-line');
        let $span = $($btn.find('span'));
        $span[0].innerText = 'COPIED';
        
        wait(function () { // 等待两秒钟后恢复
          $icon.removeClass('ri-checkbox-circle-line');
          $icon.addClass('ri-file-copy-2-line');
          $span[0].innerText = 'COPY';
        }, 2000);
      });
      clipboard.on('error', function(e) {
        e.clearSelection();
        let $btn = $(e.trigger);
        $btn.addClass('copy-failed');
        let $icon = $($btn.find('i'));
        $icon.removeClass('ri-file-copy-2-line');
        $icon.addClass('ri-time-line');
        let $span = $($btn.find('span'));
        $span[0].innerText = 'COPY FAILED';
        
        wait(function () { // 等待两秒钟后恢复
          $icon.removeClass('ri-time-line');
          $icon.addClass('ri-file-copy-2-line');
          $span[0].innerText = 'COPY';
        }, 2000);
      });
    }
    initCopyCode();
  }(window, document);
</script>
 
<!-- CanvasBackground -->
 
<script src="/js/dz.js"></script>
 
<script>
  if (window.mermaid) {
    mermaid.initialize({ theme: "forest" });
  }
</script>


    
    

  </div>
  
  <!-- 一言API -->
<!-- 现代写法，推荐 -->
<!-- 兼容低版本浏览器 (包括 IE)，可移除 -->
<script src="https://cdn.jsdelivr.net/npm/bluebird@3/js/browser/bluebird.min.js"></script>
<script src="https://cdn.jsdelivr.net/npm/whatwg-fetch@2.0.3/fetch.min.js"></script>
<!--End-->
<script>
  fetch('https://v1.hitokoto.cn')
    .then(function (res){
      return res.json();
    })
    .then(function (data) {
      var hitokoto = document.getElementById('hitokoto');
      hitokoto.innerText = data.hitokoto + '——【' + data.from + '】';
    })
    .catch(function (err) {
      console.error(err);
    })
</script>
<script src="/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"log":false,"pluginJsPath":"lib/","pluginModelPath":"assets/","pluginRootPath":"live2dw/","tagMode":false});</script></body>

</html>